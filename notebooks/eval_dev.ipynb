{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import LinearNDInterpolator\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_errors(gt, pred):\n",
    "    \"\"\" Evaluates the predicted depth data on ground truth\n",
    "\n",
    "    Args:\n",
    "        gt: numpy array (2D) of the ground depth map\n",
    "        pred: numpy array (2D) of the predicted depth\n",
    "\n",
    "    Returns:\n",
    "        -abs_rel\n",
    "        -sq_rel\n",
    "        -rmse\n",
    "        -rmse_log\n",
    "        -a1\n",
    "        -a2x\n",
    "        -a3\n",
    "    \"\"\"\n",
    "\n",
    "    thresh = np.maximum((gt / pred), (pred / gt))\n",
    "    a1 = (thresh < 1.25   ).mean()\n",
    "    a2 = (thresh < 1.25 ** 2).mean()\n",
    "    a3 = (thresh < 1.25 ** 3).mean()\n",
    "\n",
    "    rmse = (gt - pred) ** 2\n",
    "    rmse = np.sqrt(rmse.mean())\n",
    "\n",
    "    rmse_log = (np.log(gt) - np.log(pred)) ** 2\n",
    "    rmse_log = np.sqrt(rmse_log.mean())\n",
    "\n",
    "    abs_rel = np.mean(np.abs(gt - pred) / gt)\n",
    "\n",
    "    sq_rel = np.mean(((gt - pred)**2) / gt)\n",
    "\n",
    "    return abs_rel, sq_rel, rmse, rmse_log, a1, a2, a3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_data(files, data_root):\n",
    "    \"\"\" Reads in the files for evaluation of ground truth data\n",
    "\n",
    "        Args:\n",
    "            files (list string): a list of filenames (two files per line left/right)\n",
    "            data_root (string): root folder \n",
    "\n",
    "        Returns:\n",
    "            -gt_files (list string): filenames for the velodyne points (gt from laser range scanner)\n",
    "            -gt_calib (list string): path for respective camera calibration files of the particular sequence \n",
    "            -im_sizes (list tuples): sizes of each gt file for later resizing \n",
    "            -im_files (list string): list of filenames for left images\n",
    "            -cams (list int): list of camera that image was taken from (2 is left and 3 is right)\n",
    "        \"\"\"\n",
    "\n",
    "    gt_files = []\n",
    "    gt_calib = []\n",
    "    im_sizes = []\n",
    "    im_files = []\n",
    "    cams = []\n",
    "    num_probs = 0\n",
    "\n",
    "    for filename in files:\n",
    "        filename = filename.split()[0]\n",
    "        splits = filename.split('/')\n",
    "        camera_id = np.int32(splits[2][-1:])  # 2 is left, 3 is right\n",
    "        date = splits[0]\n",
    "        im_id = splits[4][:10]\n",
    "        file_root = '{}/{}'\n",
    "\n",
    "        im = filename\n",
    "        vel = '{}/{}/velodyne_points/data/{}.bin'.format(splits[0], splits[1], im_id)\n",
    "\n",
    "        if os.path.isfile(data_root + im):\n",
    "            gt_files.append(data_root + vel)\n",
    "            gt_calib.append(data_root + date + '/')\n",
    "            im_sizes.append(cv2.imread(data_root + im).shape[:2])\n",
    "            im_files.append(data_root + im)\n",
    "            cams.append(2)\n",
    "        else:\n",
    "            num_probs += 1\n",
    "            print('{} missing'.format(data_root + im))\n",
    "            \n",
    "    print (num_probs, 'files missing')\n",
    "\n",
    "    return gt_files, gt_calib, im_sizes, im_files, cams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 files missing\n"
     ]
    }
   ],
   "source": [
    "files = open('test_files.txt', 'r')\n",
    "gt_files, gt_calib, im_sizes, im_files, cams = read_file_data(files,'../../data/kitti/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_velodyne_points(file_name):\n",
    "    \"\"\" Reads in the ground truth depth file\n",
    "        # adapted from https://github.com/hunse/kitti\n",
    "\n",
    "        Args:\n",
    "            file_name(string): path to velodyne bin file to be loaded \n",
    "\n",
    "        Returns:\n",
    "            -points (2D): points \n",
    "    \"\"\"\n",
    "    \n",
    "    points = np.fromfile(file_name, dtype=np.float32).reshape(-1, 4)\n",
    "    points[:, 3] = 1.0  # homogeneous\n",
    "    return points\n",
    "\n",
    "def lin_interp(shape, xyd):\n",
    "    \"\"\" Linearly interpolates the depth data to fill zero holes\n",
    "        # adapted from https://github.com/hunse/kitti\n",
    "\n",
    "        Args:\n",
    "            -shape(tuple): size of the image\n",
    "            -xyd (np array): \n",
    "\n",
    "        Returns:\n",
    "            -points (2D): points \n",
    "    \"\"\"\n",
    "    \n",
    "    m, n = shape\n",
    "    ij, d = xyd[:, 1::-1], xyd[:, 2]\n",
    "    f = LinearNDInterpolator(ij, d, fill_value=0)\n",
    "    J, I = np.meshgrid(np.arange(n), np.arange(m))\n",
    "    IJ = np.vstack([I.flatten(), J.flatten()]).T\n",
    "    disparity = f(IJ).reshape(shape)\n",
    "    return disparity\n",
    "\n",
    "def read_calib_file(path):\n",
    "    \"\"\" Reads in in the calibration files for the camera and puts them into a dictionary\n",
    "    check here for contents of the calibration file:  https://github.com/yanii/kitti-pcl/blob/master/KITTI_README.TXT\n",
    "    adapted from https://github.com/hunse/kitti\n",
    "\n",
    "    Args:\n",
    "        -path(string): path to the calibration file\n",
    "\n",
    "    Returns:\n",
    "        -data (dict): dictionary that is storing the different calibration arrays \n",
    "    \"\"\"\n",
    "    \n",
    "    float_chars = set(\"0123456789.e+- \")\n",
    "    data = {}\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            key, value = line.split(':', 1)\n",
    "            value = value.strip()\n",
    "            data[key] = value\n",
    "            if float_chars.issuperset(value):\n",
    "                # try to cast to float array\n",
    "                try:\n",
    "                    #data[key] = np.array(map(float, value.split(' ')))\n",
    "                    data[key] = np.array([float(value) for value in value.split(' ')])\n",
    "                except ValueError:\n",
    "                    # casting error: data[key] already eq. value, so pass\n",
    "                    pass\n",
    "\n",
    "    return data\n",
    "\n",
    "def read_text_lines(file_path):\n",
    "    \"\"\" Reads in the ground truth depth file\n",
    "        # adapted from https://github.com/hunse/kitti\n",
    "\n",
    "        Args:\n",
    "            -file_path (string): path to file \n",
    "        Returns:\n",
    "            -lines(list): filenames for the data\n",
    "        \"\"\"\n",
    "    f = open(file_path, 'r')\n",
    "    lines = f.readlines()\n",
    "    f.close()\n",
    "    lines = [l.rstrip() for l in lines]\n",
    "    return lines\n",
    "\n",
    "\n",
    "\n",
    "def get_focal_length_baseline(calib_dir, cam):\n",
    "    \"\"\" Reads in the ground truth depth file\n",
    "        # adapted from https://github.com/hunse/kitti\n",
    "\n",
    "        Args:\n",
    "            -calib_dir(string): path to camera calibration folder \n",
    "            -cam (int) camera that was used (either camera 1 or camera 2)\n",
    "\n",
    "        Returns:\n",
    "            -focal length (float): focal length of the camera used \n",
    "            -baseline (float): length of the baseline of the camera used \n",
    "        \"\"\"\n",
    "    cam2cam = read_calib_file(calib_dir + 'calib_cam_to_cam.txt')\n",
    "    P2_rect = cam2cam['P_rect_02'].reshape(3,4)\n",
    "    P3_rect = cam2cam['P_rect_03'].reshape(3,4)\n",
    "\n",
    "    # cam 2 is left of camera 0  -6cm\n",
    "    # cam 3 is to the right  +54cm\n",
    "    b2 = P2_rect[0,3] / -P2_rect[0,0]\n",
    "    b3 = P3_rect[0,3] / -P3_rect[0,0]\n",
    "    baseline = b3-b2\n",
    "\n",
    "    if cam==2:\n",
    "        focal_length = P2_rect[0,0]\n",
    "    elif cam==3:\n",
    "        focal_length = P3_rect[0,0]\n",
    "\n",
    "    return focal_length, baseline\n",
    "\n",
    "\n",
    "def sub2ind(matrixSize, rowSub, colSub):\n",
    "    \"\"\" Converts an index into our number so can be acessed in an array \n",
    "    \n",
    "    Args:\n",
    "        -matrixSize (tuple): size of the matrix\n",
    "        -rowSub (int): row index\n",
    "        -colSub (int): column index\n",
    "        \n",
    "    Returns:\n",
    "        -index (int): particular index in a flat array\n",
    "    \"\"\"\n",
    "        \n",
    "    m, n = matrixSize\n",
    "    return rowSub * (n-1) + colSub - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_depth_map(calib_dir, velo_file_name, im_shape, cam=2, interp=False, vel_depth=False):\n",
    "    \"\"\" Generates a depth map from the velodyne files using the calibration files \n",
    "    \n",
    "    Args:\n",
    "        -calib_dir (string): Directory of the calibration files\n",
    "        -velo_file_name (string): Filename of the velodyne.bin file\n",
    "        -im_shape (tuple): size of the target image, to which the velodyne points are projected \n",
    "        -cam (int): indicates which cam the image was taken from to load the respective calibration file\n",
    "        \n",
    "    Returns:\n",
    "        -index (int): particular index in a flat array\n",
    "    \"\"\"\n",
    "    # load calibration files\n",
    "    cam2cam = read_calib_file(calib_dir + 'calib_cam_to_cam.txt')\n",
    "    velo2cam = read_calib_file(calib_dir + 'calib_velo_to_cam.txt')\n",
    "    velo2cam = np.hstack((velo2cam['R'].reshape(3,3), velo2cam['T'][..., np.newaxis]))\n",
    "    velo2cam = np.vstack((velo2cam, np.array([0, 0, 0, 1.0])))\n",
    "\n",
    "    # compute projection matrix velodyne->image plane\n",
    "    R_cam2rect = np.eye(4)\n",
    "    R_cam2rect[:3,:3] = cam2cam['R_rect_00'].reshape(3,3)\n",
    "    P_rect = cam2cam['P_rect_0'+str(cam)].reshape(3,4)\n",
    "    P_velo2im = np.dot(np.dot(P_rect, R_cam2rect), velo2cam)\n",
    "\n",
    "    # load velodyne points and remove all behind image plane (approximation)\n",
    "    # each row of the velodyne data is forward, left, up, reflectance\n",
    "    velo = load_velodyne_points(velo_file_name)\n",
    "    velo = velo[velo[:, 0] >= 0, :]\n",
    "\n",
    "    # project the points to the camera\n",
    "    velo_pts_im = np.dot(P_velo2im, velo.T).T\n",
    "    velo_pts_im[:, :2] = velo_pts_im[:,:2] / velo_pts_im[:,2][..., np.newaxis]\n",
    "\n",
    "    if vel_depth:\n",
    "        velo_pts_im[:, 2] = velo[:, 0]\n",
    "\n",
    "    # check if in bounds\n",
    "    # use minus 1 to get the exact same value as KITTI matlab code\n",
    "    velo_pts_im[:, 0] = np.round(velo_pts_im[:,0]) - 1\n",
    "    velo_pts_im[:, 1] = np.round(velo_pts_im[:,1]) - 1\n",
    "    val_inds = (velo_pts_im[:, 0] >= 0) & (velo_pts_im[:, 1] >= 0)\n",
    "    val_inds = val_inds & (velo_pts_im[:,0] < im_shape[1]) & (velo_pts_im[:,1] < im_shape[0])\n",
    "    velo_pts_im = velo_pts_im[val_inds, :]\n",
    "\n",
    "    # project to image\n",
    "    depth = np.zeros((im_shape))\n",
    "    depth[velo_pts_im[:, 1].astype(np.int), velo_pts_im[:, 0].astype(np.int)] = velo_pts_im[:, 2]\n",
    "\n",
    "    # find the duplicate points and choose the closest depth\n",
    "    inds = sub2ind(depth.shape, velo_pts_im[:, 1], velo_pts_im[:, 0])\n",
    "    dupe_inds = [item for item, count in Counter(inds).items() if count > 1]\n",
    "    for dd in dupe_inds:\n",
    "        pts = np.where(inds==dd)[0]\n",
    "        x_loc = int(velo_pts_im[pts[0], 0])\n",
    "        y_loc = int(velo_pts_im[pts[0], 1])\n",
    "        depth[y_loc, x_loc] = velo_pts_im[pts, 2].min()\n",
    "    depth[depth<0] = 0\n",
    "\n",
    "    if interp:\n",
    "        # interpolate the depth map to fill in holes\n",
    "        depth_interp = lin_interp(im_shape, velo_pts_im)\n",
    "        return depth, depth_interp\n",
    "    else:\n",
    "        return depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 files missing\n"
     ]
    }
   ],
   "source": [
    "files = open('test_files.txt', 'r')\n",
    "gt_files, gt_calib, im_sizes, im_files, cams = read_file_data(files,'../../data/kitti/')\n",
    "depth = generate_depth_map(gt_calib[0],gt_files[0],(375, 1242), cam=2, interp=False, vel_depth=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAACICAYAAADpjSA2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAH49JREFUeJztnXmcFOWd/9/fmQYEFRVjUM4BHVAQOYc5NOuV9f4t6Brj\nGd2YmOzqqrkMao79JTFq4i8b88uxa6KJJooSo9F44Xola7gGDCqo3MgtKCAq7MDMfPePququ6enu\n6e6p6q7q+b5fr37VU09V1/M8PdOf+tbnOVpUFcMwDKNyqSp3BQzDMIxwMaE3DMOocEzoDcMwKhwT\nesMwjArHhN4wDKPCMaE3DMOocEITehE5Q0SWichKEZkRVjmGYRhGbiSMcfQiUg0sB/4e2AA0Axep\n6huBF2YYhmHkJKyIfiqwUlVXq+pe4EFgWkhlGYZhGDkIS+gHA+t9+xvcPMMwDKPEJMpVsIhcBVwF\nUE315H70L1dVDMMwYskH7HhXVQ/r6rywhH4jMNS3P8TNS6KqdwF3AfSXAVovp4ZUFcMwjMrkOX34\n7XzOC8u6aQZqRWSEiPQGLgQeD6kswzAMIwehRPSq2ioi1wCzgWrgHlVdGkZZhmEYRm5C8+hV9Sng\nqbCubxiGYeSHzYw1DMOocEzoDcMwKhwTesMwjArHhN4wDKPCMaE3DMOocEzoDcMwKhwTesMwjArH\nhN4wDKPCMaE3DMOocEzoDcMwKhwTesMwjArHhN4wDKPCMaE3DMOocEzoDcMwKhwTesMwjArHhN4w\nDKPCMaE3DMOocEzoDcMwKpwuhV5E7hGRrSKyxJc3QET+S0RWuNtDfMduFJGVIrJMRE4Pq+KGYRhG\nfuQT0f8GOCMtbwbwvKrWAs+7+4jIGOBCYKz7np+LSHVgtTUMwzAKpkuhV9W/ANvTsqcB97rpe4Hp\nvvwHVbVFVdcAK4GpAdXVMAzDKIJiPfqBqrrZTW8BBrrpwcB633kb3DzDMAyjTHS7M1ZVFdBC3yci\nV4nIQhFZuI+W7lbDMAzDyEKxQv+OiBwB4G63uvkbgaG+84a4eZ1Q1btUdYqqTulFnyKrYRiGYXRF\nsUL/OHC5m74ceMyXf6GI9BGREUAtsKB7VTQMwzC6Q6KrE0RkJnAS8DER2QB8G7gNmCUiVwJvAxcA\nqOpSEZkFvAG0AleraltIdTcMwzDyoEuhV9WLshw6Ncv5twC3dKdShmEYRnDYzFjDMIwKx4TeMAyj\nwjGhNwzDyEH7iRPLUu6e6VPZMy2Y+aZdevSGYRhxYt9pUwDo9ezCTsf2nlEHQO9nmtl7Rh29n2nu\n8npVf/5bsBXMk75/DG7Aogm9YRgVRSaB9/ALe+9nmmk7eVLqoDvts/qlV8KqWlZ2n1dPv0fmJ/f3\nTJ8aqNBHwrrR/v3Ye/qUclfDMIwA2PmZxlCv33J2XWDXqn7xldTrpVcQddS+1HaNX+R3n1efFPk9\n04OxbiIh9LJrN71nZ78LG4YRHw6+b26H/fcvbeiwv+uiBnZd1DGvGIIUfA/PpsnXrklvWxD4RT+o\nqD4SQu/H89CiyJbrmlLpLzXlONPh3S+EG9kYRpic+8a2ZPqCN7cUfZ2Dfjevw37/mfPoPzOV54n+\nmu838sGFXQtnnyebO2zLSXrbgmL3ufXsPrc+GdF3t1M2ckKfT+dIsbx7VfeE9/A756TS/z4nx5kO\nH/vPuV2eU05W/nYiq+6fyKoHJiTz3p41DoB1v09tvXQmNvxhLJseHQPAtsdH5132sPn7F1NlowiW\n31XXKb38l3Us/2XuoOrRMYcl07OOOZw+fz48lPq9N14AGHHTXA58MLtwhhHB58OOKzLrxs7Lwgvk\n+j06n36Pzk9G9H0f615kL6oFLzwZOP1lgNZLxom2RoGs/HEDKoAotdfOZ8XP6qm9en6n81bcOwmp\nUo66rDwjCoxw2frY0Xx82lt5n1+3uI3mCdXUNvdhRV00V5NtOasOpPyR/M7PNILCwb9Ns6guaUAU\n+j8QTpSfief04UWq2mUHZ+SEfu/pU8yvD5HVP3CikJE3RPtpw4g/71/agIqwrb6d2mucYGPlvzcg\n7XDkV0onhgBbr2ni4z/t+ik8bsRW6A3DqExW/KSe2ms7P13GiR2XN3LIvdEJkvIV+sh59IZx8+rF\n3Lx6MTeueo0bV70GwHfXlL/jzege+Yj82lsaWfu9zt73um91PfihFERJ5AvBhL4HsPznmXvsV9w3\niZW/c8YLr3Y7ZNc+dFzyuNcRm6kz1uu0Bdj4yNhk+tC/HsLAuf0BqFnQN7n1Ol/9nbA1C/pyzKLO\nc/ZuGTmBW0ZO4NYjj+PWI536fHNEdEdjxY3l9zgB4PJfFTZ3peXZmmR662NHJ9NvzxrHmpnjA6lb\nzc1zqflGZzEd9p2U7dJyZub/BS+/5aw6x8/PQJRH9XXFnmnFL4nQ462bbV9s5LD/yH2Xfudfm8AZ\nGICKM/pmy/VNHP5j559v85ebOOJHXft/G7/exODb57DphiYG/aDz+f78TTc0gcKgHzr7G2c0Mfi2\nOWy4sYkht6beu/4bTQz9XmHe4+ofNJpH34NZfs8UaIdRn+u6L6zPnw+ntb2KtpM3sWf2CPqevgaA\nLX88hsOnv5n1fe9f2hDa0MNCeefaJgb+pON3ZOu/NPHxnzt5736hMfIj5LJhHn0a732+kUN/OZd3\nr2rkY3fN5d0vNDqjU6CD0G+9uomP/6xr4fQLfS48ce8qD1JCn+1GkE66yK/9XiOIExWFwYp7J1F7\neXDTw/f/y2F89Hfbuj7RiCzrHz6WoecvYfUDExh58eJOx70JRVER/SDY+ZnGTpPCykWshP7Ag4bo\npKZ/BbARNxXG+m84T0NDv1t5Ix5Kxd3rXubKYSck95/YuIhzBk8uY43CZe0tjdTcPJe3v9MIKgz/\n9hzWfaspad+0nFVHn6dSfTb+/ZYz6+jzdOb+HM/ayXY8jsRK6OMw6sazTuLAzssaO4zx3XFFI4f8\npvO+NxHEf8zDewJ673POU8Khv4xGBNPTuWr5au4aNZJTXv+IfVpNm1YxZ3zvclerLHii3nKmO77+\nqewCnn4DSLduIPoTHDMRmNCLyFDgPmAgzvpud6nqnSIyAHgIqAHWAheo6g73PTcCVwJtwLWqOjtX\nGfkK/Ufn17P/w90bnuVZN13x3pWNHHp3/P7w3SX9JmFEn11PH0n/M1cBKe984yNjaW8Xhp6/hHW/\nH8ewT72e9f0rfzuRoy77G+t+Pw5VGH7B66x96DhqPv0aq+6fyJGXVPakunShf+9z7qgfpYMGpAdM\nUSBIoT8COEJVXxGRA4FFwHTgCmC7qt4mIjOAQ1T16yIyBpgJTAUGAc8Bo3L9SHgcInqPXJ6j591l\nGmu7/bPOP8+Ae5wo+dBf5dEBDAz8/9mfIjyPfv03m4q2RtLbUyr/UV4YjJ6yMfRyKoXtT4wCYMA5\ny5N52x4fzd7WBIPPW9pBkFf8ZjK1VywqSz09ouRjR4EPPt3AgQ8V30+xZ/rU5DLK/uUQQrNuROQx\n4Kfu6yRV3ezeDF5S1dFuNI+q3uqePxv4N1XN+lePk9D3dJb/fCqj/iW4dbKN8FhxZwO11znisur/\nNXDkV+Y5M6PbYeSM1Ndx7S1OEFJz81zWfL+RETfNZe33GjMOc4w6fosm3cuvREKZMCUiNcBEYD4w\nUFU3u4e24Fg7AIOB9b63bXDzykqYCxD1JEzk44Mn8pBaciDxoTByxlzWfzM1ASnxoSRHao24ydnW\nfGOuM8QX2PS1JjZ91U1/1Umnr9665bqm5CsT71zbxDvX5j/pyX/u1muc9LZ/Tn2Hs60M6/fhM4m8\nZ8u8d2Vj8gWpJ+586Gq9/SCWYA6avIVeRA4A/gBcr6q7/MfUeSwo6NFARK4SkYUisnAfLew9fUqo\nPz7Sle+84/JGdlzRmHWluqiz66KG5BKv+Sz1GgZ+MSiUFXc2ZExDamJP+mqLy/8zvpNfgsIT4y7P\n+5pznjdyZeh357Dpa01s/oozL8P/N9tynSPK3hDfxB4YdMcctl7dxKA75jDojjkdVm/d9sVGqluU\nw++c02GFV4/3Pt/IwJ/MSY5l98QVUgKbLrT+ce/eGjWH/SL1HU766VcWINCXOZbp+5c2cOjdc5Mv\ncCxVjw8+3cCHFzTw/iXO/+GuixvYdXEDH51fD0DVvo5Sl/598y/BHBXysm5EpBfwBDBbVX/k5i3D\nrJu8yebRZ5ussfWaJlRSHv2W65qciVpfasprieSewOrbG5PhhTslghGuJeFZEB7p+0Z47LysEVFN\n9vvkM3nq/UsaOOj+6Alk1AmyM1aAe3E6Xq/35f8QeM/XGTtAVW8QkbHAA6Q6Y58HavPtjM13rOuH\nFzRwwCznH8Pr6Pjgwoac61kb8cU/jho6TxbbcFMTQ74/J7k1eh6eJ99yVh1oZY2Xz0aQHv3xwGXA\nKSKy2H2dBdwG/L2IrAA+6e6jqkuBWcAbwDPA1blEPp0+Tzfn9QfyRB5I9mYXKvI7L2vM6N0X4tcZ\npaHXR6n05i83ddjf8qUmEnucdGJPfr/+1RPxrIhKxfPk+zyVn4b0JGzCVBFkexTdcXkjop1/M7NU\neE80fs8w6CccL2L2PN1Bd5Q2evY65Dy/Nn3fyzvsF3NT2zzWMzKMIGk5sw7xSWtYv5wXy5mxe8+o\n6/ID+fBT9akdcZxZf3QfdbZ/trFDxw84k7i0GlQkOVsPivuxhGzjdfOxtbyOrVwTxfzr9Gz6alPJ\nhT4T6TN8/R3q/hnA/v1c+UbPovXUySSeX5R9e0pquYnEC+Wdn5BOLIW+p/LuFxrRqo5RKaSGlQGh\n/TrOroud6D/Xz5/5Zwn7o2OvIznoKeT+jjnPbkjuu09TnbbWmWcERNvJk5Kd/NUvpRbxaztpUqe8\ncuDVA+DFF28yoQ+aSlyJr1i2/1MjA36diqCjGBH7O+wL2f/wgpT1FaenxTDZM31q8oeqvfSe6am1\n0fv+cQF7pk3N+iPWLWfWoQlhvz91PN5ytttx+lRzyu5Q7bC44b7THB3r9awteKjHT0D+mloltGIj\nes+6OeD30f1JsuRSCDkWDQuC7f/kDtn8denF1ZsUUu4xw7vPrQeBfo+k/h92n1dPv0fmF7w1iqPl\n7Dr6PNnc4cc+/KtJQmoETKbfhPYskp6GNnb+sRaZ+2pB16hYoe8u/o5U/4ibIBby8kf8pV4cbNdF\nDfSfOY9dFzdktGGC/CGIj/6xnv3/ML9TupT8z/+Zyn5/WtBpm37cSwOdokkjWNJF3JsA2Z2lx9tP\nnEjVn2O8qFrDcTDvtVS6XWFB9gXmCqWihN4vJh9+qj7S0XwxhB0de1aEZ0OE9Rn6RRfiI6y51jBP\nPw86js/usLaK73hYa5+n1zW9zD5PNyd/Ls8/sGHfaVOgXRGlQ/S877Qp9Hp2Ifs+6XQ4esdbT5nc\nrY7H9k9MpOq/YyzQLl7UXWikXSoq6sfB/RGjJ1B+HzUsPvxUffLl7QfNR+fX03/mvFAtkANmzUtO\n2959bj1VreGUI61Ky5l17PenBZ2miUeNfadNSXq/fZ5u7rDvpdP3q9q007nevpeuatOM5wa1vEfV\nvvZknbwyPeH3yuv9TDO9n2lm3ycnpwS8Ten13KJOFkl1izPFRdqdV+L5RbSdPCkp8u0nTqT9xImd\n0l3Ws0Qin16f9k+k9ttPmJDzvZmsk3Rk7qtlF3mZPBaZPLbrE3NdIw4RfaGE4bnuPreefo/OD+36\nXgfX7vOcm0nQ1/f7qN7swSBX9vMEpddz8fRa033i9IjW2/dvPRIvOEPxvJEa3vu8c5LHARREleoX\nM4/caDt5EtUvvpIa+SEkz207aVKHER+Z6pTr2tmIS/Td/omJoErVy4vR4x0R93dMauN4R5Qb3B+4\n9yyTiCMTx6J/W1rUeyvKuokD/l9nzzbyoLu0nO0+pj/ZnBTu7pJ8dHe3pcAvoulpiN5Y5VLRfsIE\nql5enLfwatN4ZE40LYWehEw5Fl24pCxlx9a68Vax9HzGsPBE0+N/zpma5cz86PvYguSrq7KKpc+T\nzUlxl9ZgbtCeuHtbT2z9EWvQ+G0Bv6gnXlhE4oVFedsDcceLSrXJ9YHb1RHvdk0dOz63/QCkItgI\no43jO1slGeotdeOQKceWqFbBUEqRrxp/THHvC7ge3ab37IXOK8sMWf8NwOuIKob0aLgqTTiDvNH0\nedLpIAvymmF54J7Qhim4nnfqjaZI91LLMcpCj5/QQVzT9zOdl37Mv59r670828GLymXOq87rr4tT\nx3zWBKRuCt75+fjMZcUVc1FF0t2Dea/B1HEdsrT59cCFU+rGZUwn81z/O7md2D0/vBCqJowpSLzb\nX32zqHIq0rqJ0wSLoKdX+8U5SMHM5Il263peFDvn1aQF4Rcx75gRA6aOgwWvJ0VUm1PDBz1bI30b\nFH7h9pdbCmTyWHTR0m557N2lIjz6oCZSeB1hYUzKCPLa3pjh7o4dTkbMLxcuyv7hZMnOrTLhj1aj\nUJ8egyvcXlrUEVGpGweuXujCJUmhiypRr1++VB13NLSn9tuXvJVMx9aj9xOUMCee7zysLJ22kyfl\nPJ7r2lWt7V2fmA9t2nFbBNo4nqqXFxcl8tBxOFlGUS2hHyxzX0Xa2pG2dkdw5r6aetRP31YAnmUg\nE8em0iHaCV403MHamHJsUuS9dDJSbm9HFy5JRuTlFNGqCWM62B6Z7A9dtJSqY48Orsy0sqqOO7rD\nKyykTWlf8lbyVdQ1ohzRF4N/wZ9yLz7UXbsjsAjWH6Fl2g8QqRsX2CO0/zG/nCMbKhnvRqKLlpYt\nAq4af0wn77nquKNpf604UcurzAljAGhf/EZoZZSC2Fs3QY3t9U+gKPZ66QsJdXl+GBZDiOJcCFF7\nHPbXJ+mZptUxanWOAt4TQrm85XzwovFio9hSkm6veIgqbUuXhVZu7IW+7AQ56SIgkfaGnRXSqeWf\nURem2PmFo5ydU0Y8CDtiz4fqMaOS6bY3lqfyRx9F27KVWd9XdezRtC95K7ktJxUv9P6RGmUbnRGA\ngPvHDOdrTQQZoVZNGNPh8bVSHmk90m0Bb9/v6Xr7mfLLUcdArpmnCFUfU0vbmysCLbscVI8Z1UGs\nA7vu2NEdIvL0/aKvG1B9g/xx8P2AvwB9gATwsKp+W0QGAA8BNcBa4AJV3eG+50bgSqANuFZVZ+cq\no/8Bg7X+uC+WZcqyf0hYd/zlQiLnMCLedMGOAl4HVftrb0UigjOCIz2qjVKUGxSZIvvqUUcm023L\nVxV+Te8pQjWQG2yQQi/A/qr6oYj0Al4GrgPOA7ar6m0iMgM4RFW/LiJjgJnAVGAQ8BwwKtcPhPeX\nAVpff72zU2YfuhjBrgSrIg5f0PQ6+j1c/+iKqLejHHiRaK5IsivLopz46x3lp5DqMaMCE/Gc5bg3\nnNnLbg/euhGRfjhC/8/AfcBJqrpZRI4AXlLV0W40j6re6r5nNvBvqpp1cfZ8rJtiLI5O1yixIHs2\nQKksgE7ll7EzKyksvkfdoB57jfLjCW/6Nup4AllMNB4mxd5kA/XoRaQaWAQcBfzMjdx3qurB7nEB\ndqjqwSLyU2Ceqv7OPXY38LSqPpzt+pHsjC0z5RTFcgtyumh4j7txEJK4E+movnYkbStWl7sakSLQ\nCVOq2qaqE4AhwFQROTbtuJJcpDU/ROQqEVkoIgv30ZKqkDsRImelxx+TfBWL5x+HOdHBT/XY0VSP\nHZ33+cUKbfWYUR1GExRDvmV75XS3vE7lpwl62xvLOwr/MbVUH1PbsS7ufnq+4eD3lnMRtMhXjz4q\n534hdFfkq2tHduv9QZIYWUNixHASI4aXpLyCR92IyLeA3cDnKaF1Uwh+8Q67A9AT7zAiYL8XGbYv\n6QlkGGWUsh1GuEQh4i91ZF991AjaVq4hMbKG1tVrS1ZuPgTZGXsYsE9Vd4pIX+BZ4HbgROA9X2fs\nAFW9QUTGAg+Q6ox9HqjtsjO2C6EvVWdhmF5jWGIa1pfPi77K/sV22xcFkakE/JFtOa2Q6lFHRs4r\nDxLvBhEmQQr9ccC9QDWO1TNLVb8jIocCs4BhwNs4wyu3u++5Gfgs0Apcr6pP5yojrh591KPTUgi1\nv3Or0r+4Rm68G0gUfPTEiOG0rnk7lOsCoVy7GCp+wlSokXcEI8cwRDTqwux/RI+SiFQCiZE1AJGz\nIjxKEQ1nImh7JqwbjkfFCH3QUXOQIh6EUIbpN5Z7lEJ655eJtBE2pfTREzXDAGhdu6771yrySaEi\nlimG/P3sfHvz8xH5vEcoBBANhyl+xV67+qgRya2XLrZ8/6uY8oOoh9ERL5ov1YiPXHUImmJFvpjP\nonXtukBEHhyBDzPyj7zQBzUsrJChVaWwM8IWrsTImqLL8B6Z21auKejxOag2+csvpB6JkTWdBCQp\naiEJS9j42+RvQ8Y8V6y8SDMbnhiW02fubtQd9NDEYj6LRM2wDq+gCPJaHpGwbg7a7widuveEclcj\nMErhL5biETXMMvwesV+souoZ90QSNcMCi1gzXj9k/9orAwDVUNuSV12GDwWg9e31gV2zYjz6qFCu\nzqHu4Al1WIJd6nHFfh/TH815+1EZCVEq/G320tl848TwoSBSdrELgp74t85GrDx66dO74PeU2rMt\nVuTD8EHzvWbyET2DGAfx6JtL5NOvHcTn4PcxvbR/Pxe52pu0PDLUOai/XyGfR95/X1+bk59DFt+4\n9e313RJ57wYShq1QKMXaLJVGYuiQvM+1iD4iRG18bjYSNcOcyDDEevotg7Dtg55IYvjQQO2DcuAX\n7rj9fySGDKZ1w8ZArhWriL67xLWjzU9Xve5BRSTd7TxqXbsuaz2D6pjyf3HD+BInaoaRGD60Qz3T\n65xPG7K9Pz3tLyv5Gbl+LZA1HRZRF3nvM0gMH5r18/CeXOIm8kBgIl8IkY7o4xLlFkOQEUnQUW+Q\n44PzKs+NMMPorDK6JjF0CK3rN6T2A4w4S02QTyvpn0sQ10M10M+2x3fGBiFW3RXQqAlX1OpTTvyR\noneT8d9svHzv3GI/M/9nnqnM9LLiQhxvBkELdxTo8UIfJlHxOMOqh1/0St3OSvwyRp3EkMFA6SyF\nMMsLPAoP+IYW9PV6lEdfasIUv8TQIXn3pudbD+96hV63kOsXUu+cZZdI5LPV1d+O9DZlSmfaZvq8\ns10riM+su7Ru2Fi0+HiiHVZ5hV6/kP8f79q5yijkc0kMHkRi8KCc5+Td7i6uUygW0RdJdyOHMPy/\nsESy3FG2J4YW6efGf9OI0mflRbFxtHuiiHcTaN24ySL6sCOlXF+kfMrO94uYb0STz/USQwYXF4EV\nKRrFlpep/ELrkB6t5RO9lYtMdfPnFfI/UMxnVQyFRJyeuFeyyOcTzQdF68ZNtG7cVNB7LKIvkCCi\nkrhFNqF6qhbtlZ3EEYfTunlLMg0k9+OAX2ALFcBSXjMMYtUZKyIfAOX7Nerw+BjwbrkrEQLWrnhh\n7YoXhbRruKoe1tVJie7VJzCW5XNXihsistDaFR+sXfHC2pU/FevRG4ZhGA4m9IZhGBVOVIT+rnJX\nICSsXfHC2hUvrF15EonOWMMwDCM8ohLRG4ZhGCFRdqEXkTNEZJmIrBSRGeWuTyGIyFAReVFE3hCR\npSJynZs/QET+S0RWuNtDfO+50W3rMhE5vXy1z42IVIvI30TkCXe/Etp0sIg8LCJvicibItJYIe36\nkvv/t0REZorIfnFsl4jcIyJbRWSJL6/gdojIZBF53T32ExGRUrfFT5Z2/dD9P3xNRB4VkYN9x4Jv\nl6qW7QVUA6uAkUBv4FVgTDnrVGD9jwAmuekDgeXAGOAHwAw3fwZwu5se47axDzDCbXt1uduRpW1f\nBh4AnnD3K6FN9wKfc9O9gYPj3i5gMLAG6OvuzwKuiGO7gL8DJgFLfHkFtwNYADQAAjwNnBnBdp0G\nJNz07WG3q9wR/VRgpaquVtW9wIPAtDLXKW9UdbOqvuKmPwDexPniTcMRFdztdDc9DXhQVVtUdQ2w\nEucziBQiMgQ4G/iVLzvubToI5wt3N4Cq7lXVncS8XS4JoK+IJIB+wCZi2C5V/QuwPS27oHaIyBFA\nf1Wdp4463ud7T1nI1C5VfVZVW93deYC3bkoo7Sq30A8G/EskbnDzYoeI1AATgfnAQFXd7B7aAgx0\n03Fp74+BG4B2X17c2zQC2Ab82rWkfiUi+xPzdqnqRuAOYB2wGXhfVZ8l5u3yUWg7Brvp9Pwo81mc\nCB1Cale5hb4iEJEDgD8A16vqLv8x9+4bm6FNInIOsFVVF2U7J25tckngPD7/QlUnAh/hWAFJ4tgu\n17OehnMjGwTsLyKX+s+JY7syUSnt8CMiNwOtwP1hllNuod8I+H8UcoibFxtEpBeOyN+vqo+42e+4\nj1q4261ufhzaezzwDyKyFsdKO0VEfke82wROBLRBVee7+w/jCH/c2/VJYI2qblPVfcAjQBPxb5dH\noe3YSMoG8edHDhG5AjgHuMS9iUFI7Sq30DcDtSIyQkR6AxcCj5e5Tnnj9nrfDbypqj/yHXocuNxN\nXw485su/UET6iMgIoBangyUyqOqNqjpEVWtw/h4vqOqlxLhNAKq6BVgvIqPdrFOBN4h5u3AsmwYR\n6ef+P56K01cU93Z5FNQO1+bZJSIN7ufxGd97IoOInIFjj/6Dqu72HQqnXeXsjXZvYmfhjFZZBdxc\n7voUWPcTcB4lXwMWu6+zgEOB54EVwHPAAN97bnbbuowyjwbIo30nkRp1E/s2AROAhe7f64/AIRXS\nrv8LvAUsAX6LM2Ijdu0CZuL0M+zDeQK7sph2AFPcz2IV8FPciaERa9dKHC/e043/CLNdNjPWMAyj\nwim3dWMYhmGEjAm9YRhGhWNCbxiGUeGY0BuGYVQ4JvSGYRgVjgm9YRhGhWNCbxiGUeGY0BuGYVQ4\n/wuM4jgiYhcPmQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xb28219128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(depth)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_eigen_split(predicted_disp_path, gt_path, crop, min_depth, max_depth):\n",
    "    \"\"\" Evaluates the predicted depth data for the KITI dataset on 697 images of the Eigensplit, by using the Velodyne\n",
    "    points for Reprojection\n",
    "\n",
    "    Args:\n",
    "        -predicted_disp_path (string): path for the predicted disparities\n",
    "        -gt_path (string: path for the ground truth\n",
    "        -crop (string): either 'eigen' or 'garg' depending on which crop to use \n",
    "        -min_depth (int): minimal depth that will be used \n",
    "        -max_depth (int): maximal depth that will be used \n",
    "\n",
    "    Returns:\n",
    "        -abs_rel\n",
    "        -sq_rel\n",
    "        -rmse\n",
    "        -rmse_log\n",
    "        -a1\n",
    "        -a2\n",
    "        -a3\n",
    "    \"\"\"\n",
    "    \n",
    "    pred_disparities = np.load(predicted_disp_path)\n",
    "    \n",
    "    num_samples = 697\n",
    "    test_files = read_text_lines(self.gt_path + 'eigen_test_files.txt')\n",
    "    gt_files, gt_calib, im_sizes, im_files, cams = read_file_data(test_files, self.gt_path)\n",
    "\n",
    "    num_test = len(im_files)\n",
    "    gt_depths = []\n",
    "    pred_depths = []\n",
    "\n",
    "    for t_id in range(num_samples):\n",
    "        #generate the depth map from the ground truth velodyne laser scans \n",
    "        camera_id = cams[t_id]  # 2 is left, 3 is right\n",
    "        depth = generate_depth_map(gt_calib[t_id], gt_files[t_id], im_sizes[t_id], camera_id, False, True)\n",
    "        gt_depths.append(depth.astype(np.float32))\n",
    "        \n",
    "        #scale the predicted disparity map to the size of the ground truth and match the scale of the disparities\n",
    "        disp_pred = cv2.resize(pred_disparities[t_id], (im_sizes[t_id][1], im_sizes[t_id][0]), interpolation=cv2.INTER_LINEAR)\n",
    "        disp_pred = disp_pred * disp_pred.shape[1]\n",
    "\n",
    "        # convert from disparity to depth, by the depth formula (baseline*focal_length / disp_pred)\n",
    "        focal_length, baseline = get_focal_length_baseline(gt_calib[t_id], camera_id)\n",
    "        depth_pred = (baseline * focal_length) / disp_pred\n",
    "        depth_pred[np.isinf(depth_pred)] = 0\n",
    "\n",
    "        pred_depths.append(depth_pred)\n",
    "    \n",
    "    #initialize arrays for all the errors \n",
    "    rms     = np.zeros(num_samples, np.float32)\n",
    "    log_rms = np.zeros(num_samples, np.float32)\n",
    "    abs_rel = np.zeros(num_samples, np.float32)\n",
    "    sq_rel  = np.zeros(num_samples, np.float32)\n",
    "    d1_all  = np.zeros(num_samples, np.float32)\n",
    "    a1      = np.zeros(num_samples, np.float32)\n",
    "    a2      = np.zeros(num_samples, np.float32)\n",
    "    a3      = np.zeros(num_samples, np.float32)\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        gt_depth = gt_depths[i]\n",
    "        pred_depth = pred_depths[i]\n",
    "        \n",
    "        pred_depth[pred_depth < min_depth] = min_depth\n",
    "        pred_depth[pred_depth > max_depth] = max_depth\n",
    "        \n",
    "        #only use those pixels that are within the specified depth range\n",
    "        mask = np.logical_and(gt_depth > min_depth, gt_depth < max_depth)\n",
    "        \n",
    "        #use different crops \n",
    "        if self.crop == 'garg' or self.crop == 'eigen':\n",
    "            gt_height, gt_width = gt_depth.shape\n",
    "\n",
    "            # crop used by Garg ECCV16\n",
    "            # if used on gt_size 370x1224 produces a crop of [-218, -3, 44, 1180]\n",
    "            if self.crop == 'garg':\n",
    "                self.crop = np.array([0.40810811 * gt_height,  0.99189189 * gt_height,   \n",
    "                                     0.03594771 * gt_width,   0.96405229 * gt_width]).astype(np.int32)\n",
    "                # crop we found by trial and error to reproduce Eigen NIPS14 results\n",
    "            elif self.crop == 'eigen':\n",
    "                self.crop = np.array([0.3324324 * gt_height,  0.91351351 * gt_height,   \n",
    "                                     0.0359477 * gt_width,   0.96405229 * gt_width]).astype(np.int32)\n",
    "            \n",
    "            #apply a mask to look only at certain pixels that lie within the crop and are valid pixels to evaluate\n",
    "            crop_mask = np.zeros(mask.shape)\n",
    "            crop_mask[self.crop[0]:self.crop[1],self.crop[2]:self.crop[3]] = 1\n",
    "            mask = np.logical_and(mask, crop_mask)\n",
    "            \n",
    "        abs_rel[i], sq_rel[i], rms[i], log_rms[i], a1[i], a2[i], a3[i] = compute_errors(gt_depth[mask], pred_depth[mask])\n",
    "        \n",
    "    return abs_rel, sq_rel, rms, log_rms, a1, a2, a3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import LinearNDInterpolator\n",
    "#from eval.eval_utils import compute_errors\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "class EvaluateEigen():\n",
    "    \"\"\"\n",
    "    Class that evaluates the KITTI data set on the Eigensplit\n",
    "\n",
    "    How to use this class:\n",
    "    Create an object of the type EvaluateEigen() and then use the evaluate method to evaluate\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,predicted_disp_path,gt_path,test_file_path,crop = 'None', min_depth=0,max_depth=80):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            -predicted_disp_path (string): path for the predicted disparities\n",
    "            -gt_path (string: path for the ground truth\n",
    "            -test_file_path (string): path where the 'eigen_test_files.txt' can be found\n",
    "            -crop (string): either 'eigen' or 'garg' depending on which crop to use\n",
    "            -min_depth (int): minimal depth that will be used\n",
    "            -max_depth (int): maximal depth that will be used\n",
    "        \"\"\"\n",
    "        super(EvaluateEigen, self).__init__()\n",
    "        self.predicted_disp_path = predicted_disp_path\n",
    "        self.gt_path = gt_path\n",
    "        self.test_file_path = test_file_path\n",
    "        self.min_depth = min_depth\n",
    "        self.max_depth = max_depth\n",
    "        self.crop = crop\n",
    "\n",
    "    def evaluate(self):\n",
    "        \"\"\" Evaluates the predicted depth data for the KITI dataset on 697 images of the Eigensplit, by using the Velodyne\n",
    "        points for Reprojection\n",
    "\n",
    "\n",
    "        Returns:\n",
    "            -abs_rel\n",
    "            -sq_rel\n",
    "            -rmse\n",
    "            -rmse_log\n",
    "            -a1\n",
    "            -a2\n",
    "            -a3\n",
    "        \"\"\"\n",
    "\n",
    "        pred_disparities = np.load(self.predicted_disp_path)\n",
    "\n",
    "        num_samples = 3\n",
    "        test_files = self.__read_text_lines(self.test_file_path + 'eigen_test_files.txt')\n",
    "        gt_files, gt_calib, im_sizes, im_files, cams = self.__read_file_data(test_files, self.gt_path)\n",
    "\n",
    "        num_test = len(im_files)\n",
    "        gt_depths = []\n",
    "        pred_depths = []\n",
    "\n",
    "        for t_id in range(num_samples):\n",
    "            #generate the depth map from the ground truth velodyne laser scans\n",
    "            camera_id = cams[t_id]  # 2 is left, 3 is right\n",
    "            depth = self.__generate_depth_map(gt_calib[t_id], gt_files[t_id], im_sizes[t_id], camera_id, False, True)\n",
    "            gt_depths.append(depth.astype(np.float32))\n",
    "\n",
    "            #scale the predicted disparity map to the size of the ground truth and match the scale of the disparities\n",
    "            disp_pred = cv2.resize(pred_disparities[t_id], (im_sizes[t_id][1], im_sizes[t_id][0]), interpolation=cv2.INTER_LINEAR)\n",
    "            disp_pred = disp_pred * disp_pred.shape[1]\n",
    "\n",
    "            # convert from disparity to depth, by the depth formula (baseline*focal_length / disp_pred)\n",
    "            focal_length, baseline = self.__get_focal_length_baseline(gt_calib[t_id], camera_id)\n",
    "            depth_pred = (baseline * focal_length) / disp_pred\n",
    "            depth_pred[np.isinf(depth_pred)] = 0\n",
    "\n",
    "            pred_depths.append(depth_pred)\n",
    "\n",
    "        #initialize arrays for all the errors\n",
    "        rms     = np.zeros(num_samples, np.float32)\n",
    "        log_rms = np.zeros(num_samples, np.float32)\n",
    "        abs_rel = np.zeros(num_samples, np.float32)\n",
    "        sq_rel  = np.zeros(num_samples, np.float32)\n",
    "        d1_all  = np.zeros(num_samples, np.float32)\n",
    "        a1      = np.zeros(num_samples, np.float32)\n",
    "        a2      = np.zeros(num_samples, np.float32)\n",
    "        a3      = np.zeros(num_samples, np.float32)\n",
    "\n",
    "        for i in range(num_samples):\n",
    "            gt_depth = gt_depths[i]\n",
    "            pred_depth = pred_depths[i]\n",
    "\n",
    "            pred_depth[pred_depth < self.min_depth] = self.min_depth\n",
    "            pred_depth[pred_depth > self.max_depth] = self.max_depth\n",
    "\n",
    "            #only use those pixels that are within the specified depth range\n",
    "            mask = np.logical_and(gt_depth > self.min_depth, gt_depth < self.max_depth)\n",
    "\n",
    "            #use different crops\n",
    "            if self.crop == 'garg' or self.crop == 'eigen':\n",
    "                gt_height, gt_width = gt_depth.shape\n",
    "\n",
    "                # crop used by Garg ECCV16\n",
    "                # if used on gt_size 370x1224 produces a crop of [-218, -3, 44, 1180]\n",
    "                if self.crop == 'garg':\n",
    "                    self.crop = np.array([0.40810811 * gt_height,  0.99189189 * gt_height,\n",
    "                                         0.03594771 * gt_width,   0.96405229 * gt_width]).astype(np.int32)\n",
    "                    # crop we found by trial and error to reproduce Eigen NIPS14 results\n",
    "                elif self.crop == 'eigen':\n",
    "                    self.crop = np.array([0.3324324 * gt_height,  0.91351351 * gt_height,\n",
    "                                         0.0359477 * gt_width,   0.96405229 * gt_width]).astype(np.int32)\n",
    "\n",
    "                #apply a mask to look only at certain pixels that lie within the crop and are valid pixels to evaluate\n",
    "                crop_mask = np.zeros(mask.shape)\n",
    "                crop_mask[self.crop[0]:self.crop[1],self.crop[2]:self.crop[3]] = 1\n",
    "                mask = np.logical_and(mask, crop_mask)\n",
    "\n",
    "            abs_rel[i], sq_rel[i], rms[i], log_rms[i], a1[i], a2[i], a3[i] = compute_errors(gt_depth[mask], pred_depth[mask])\n",
    "\n",
    "        return abs_rel, sq_rel, rms, log_rms, a1, a2, a3\n",
    "\n",
    "    def __read_file_data(self, files, data_root):\n",
    "        \"\"\" Reads in the files for evaluation of ground truth data\n",
    "\n",
    "            Args:\n",
    "                files (list string): a list of filenames (two files per line left/right)\n",
    "                data_root (string): root folder\n",
    "\n",
    "            Returns:\n",
    "                -gt_files (list string): filenames for the velodyne points (gt from laser range scanner)\n",
    "                -gt_calib (list string): path for respective camera calibration files of the particular sequence\n",
    "                -im_sizes (list tuples): sizes of each gt file for later resizing\n",
    "                -im_files (list string): list of filenames for left images\n",
    "                -cams (list int): list of camera that image was taken from (2 is left and 3 is right)\n",
    "            \"\"\"\n",
    "\n",
    "        gt_files = []\n",
    "        gt_calib = []\n",
    "        im_sizes = []\n",
    "        im_files = []\n",
    "        cams = []\n",
    "        num_probs = 0\n",
    "\n",
    "        for filename in files:\n",
    "            filename = filename.split()[0]\n",
    "            splits = filename.split('/')\n",
    "            date = splits[0]\n",
    "            im_id = splits[4][:10]\n",
    "\n",
    "            im = filename\n",
    "            vel = '{}/{}/velodyne_points/data/{}.bin'.format(splits[0], splits[1], im_id)\n",
    "\n",
    "            if os.path.isfile(data_root + im):\n",
    "                gt_files.append(data_root + vel)\n",
    "                gt_calib.append(data_root + date + '/')\n",
    "                im_sizes.append(cv2.imread(data_root + im).shape[:2])\n",
    "                im_files.append(data_root + im)\n",
    "                cams.append(2)\n",
    "            else:\n",
    "                num_probs += 1\n",
    "                print('{} missing'.format(data_root + im))\n",
    "\n",
    "        print(num_probs, 'files missing')\n",
    "\n",
    "        return gt_files, gt_calib, im_sizes, im_files, cams\n",
    "\n",
    "    def __load_velodyne_points(self, file_name):\n",
    "        \"\"\" Reads in the ground truth depth file\n",
    "            # adapted from https://github.com/hunse/kitti\n",
    "\n",
    "            Args:\n",
    "                file_name(string): path to velodyne bin file to be loaded\n",
    "\n",
    "            Returns:\n",
    "                -points (2D): points\n",
    "        \"\"\"\n",
    "\n",
    "        points = np.fromfile(file_name, dtype=np.float32).reshape(-1, 4)\n",
    "        points[:, 3] = 1.0  # homogeneous\n",
    "        return points\n",
    "\n",
    "    def __lin_interp(self, shape, xyd):\n",
    "        \"\"\" Linearly interpolates the depth data to fill zero holes # adapted from https://github.com/hunse/kitti\n",
    "\n",
    "            Args:\n",
    "                -shape(tuple): size of the image\n",
    "                -xyd (np array):\n",
    "\n",
    "            Returns:\n",
    "                -points (2D): points\n",
    "        \"\"\"\n",
    "\n",
    "        m, n = shape\n",
    "        ij, d = xyd[:, 1::-1], xyd[:, 2]\n",
    "        f = LinearNDInterpolator(ij, d, fill_value=0)\n",
    "        J, I = np.meshgrid(np.arange(n), np.arange(m))\n",
    "        IJ = np.vstack([I.flatten(), J.flatten()]).T\n",
    "        disparity = f(IJ).reshape(shape)\n",
    "        return disparity\n",
    "\n",
    "    def __read_calib_file(self, path):\n",
    "        \"\"\" Reads in in the calibration files for the camera and puts them into a dictionary\n",
    "        check here for contents of the calibration file:  https://github.com/yanii/kitti-pcl/blob/master/KITTI_README.TXT\n",
    "        adapted from https://github.com/hunse/kitti\n",
    "\n",
    "        Args:\n",
    "            -path(string): path to the calibration file\n",
    "\n",
    "        Returns:\n",
    "            -data (dict): dictionary that is storing the different calibration arrays\n",
    "        \"\"\"\n",
    "\n",
    "        float_chars = set(\"0123456789.e+- \")\n",
    "        data = {}\n",
    "        with open(path, 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                key, value = line.split(':', 1)\n",
    "                value = value.strip()\n",
    "                data[key] = value\n",
    "                if float_chars.issuperset(value):\n",
    "                    # try to cast to float array\n",
    "                    try:\n",
    "                        # data[key] = np.array(map(float, value.split(' ')))\n",
    "                        data[key] = np.array([float(value) for value in value.split(' ')])\n",
    "                    except ValueError:\n",
    "                        # casting error: data[key] already eq. value, so pass\n",
    "                        pass\n",
    "\n",
    "        return data\n",
    "\n",
    "    def __get_focal_length_baseline(self,calib_dir, cam):\n",
    "        \"\"\" Reads in the ground truth depth file\n",
    "            # adapted from https://github.com/hunse/kitti\n",
    "\n",
    "            Args:\n",
    "                -calib_dir(string): path to camera calibration folder\n",
    "                -cam (int) camera that was used (either camera 1 or camera 2)\n",
    "\n",
    "            Returns:\n",
    "                -focal length (float): focal length of the camera used\n",
    "                -baseline (float): length of the baseline of the camera used\n",
    "            \"\"\"\n",
    "\n",
    "        cam2cam = self.__read_calib_file(calib_dir + 'calib_cam_to_cam.txt')\n",
    "        P2_rect = cam2cam['P_rect_02'].reshape(3, 4)\n",
    "        P3_rect = cam2cam['P_rect_03'].reshape(3, 4)\n",
    "\n",
    "        # cam 2 is left of camera 0  -6cm\n",
    "        # cam 3 is to the right  +54cm\n",
    "        b2 = P2_rect[0, 3] / -P2_rect[0, 0]\n",
    "        b3 = P3_rect[0, 3] / -P3_rect[0, 0]\n",
    "        baseline = b3 - b2\n",
    "\n",
    "        if cam == 2:\n",
    "            focal_length = P2_rect[0, 0]\n",
    "        elif cam == 3:\n",
    "            focal_length = P3_rect[0, 0]\n",
    "\n",
    "        return focal_length, baseline\n",
    "\n",
    "    def __sub2ind(self,matrixSize, rowSub, colSub):\n",
    "        \"\"\" Converts an index into our number so can be acessed in an array\n",
    "\n",
    "        Args:\n",
    "            -matrixSize (tuple): size of the matrix\n",
    "            -rowSub (int): row index\n",
    "            -colSub (int): column index\n",
    "\n",
    "        Returns:\n",
    "            -index (int): particular index in a flat array\n",
    "        \"\"\"\n",
    "\n",
    "        m, n = matrixSize\n",
    "        return rowSub * (n - 1) + colSub - 1\n",
    "\n",
    "    def __generate_depth_map(self,calib_dir, velo_file_name, im_shape, cam=2, interp=False, vel_depth=False):\n",
    "        \"\"\" Generates a depth map from the velodyne files using the calibration files\n",
    "\n",
    "        Args:\n",
    "            -calib_dir (string): Directory of the calibration files\n",
    "            -velo_file_name (string): Filename of the velodyne.bin file\n",
    "            -im_shape (tuple): size of the target image, to which the velodyne points are projected\n",
    "            -cam (int): indicates which cam the image was taken from to load the respective calibration file\n",
    "\n",
    "        Returns:\n",
    "            -index (int): particular index in a flat array\n",
    "        \"\"\"\n",
    "        # load calibration files\n",
    "        cam2cam = self.__read_calib_file(calib_dir + 'calib_cam_to_cam.txt')\n",
    "        velo2cam = self.__read_calib_file(calib_dir + 'calib_velo_to_cam.txt')\n",
    "        velo2cam = np.hstack((velo2cam['R'].reshape(3, 3), velo2cam['T'][..., np.newaxis]))\n",
    "        velo2cam = np.vstack((velo2cam, np.array([0, 0, 0, 1.0])))\n",
    "\n",
    "        # compute projection matrix velodyne->image plane\n",
    "        R_cam2rect = np.eye(4)\n",
    "        R_cam2rect[:3, :3] = cam2cam['R_rect_00'].reshape(3, 3)\n",
    "        P_rect = cam2cam['P_rect_0' + str(cam)].reshape(3, 4)\n",
    "        P_velo2im = np.dot(np.dot(P_rect, R_cam2rect), velo2cam)\n",
    "\n",
    "        # load velodyne points and remove all behind image plane (approximation)\n",
    "        # each row of the velodyne data is forward, left, up, reflectance\n",
    "        velo = self.__load_velodyne_points(velo_file_name)\n",
    "        velo = velo[velo[:, 0] >= 0, :]\n",
    "\n",
    "        # project the points to the camera\n",
    "        velo_pts_im = np.dot(P_velo2im, velo.T).T\n",
    "        velo_pts_im[:, :2] = velo_pts_im[:, :2] / velo_pts_im[:, 2][..., np.newaxis]\n",
    "\n",
    "        if vel_depth:\n",
    "            velo_pts_im[:, 2] = velo[:, 0]\n",
    "\n",
    "        # check if in bounds\n",
    "        # use minus 1 to get the exact same value as KITTI matlab code\n",
    "        velo_pts_im[:, 0] = np.round(velo_pts_im[:, 0]) - 1\n",
    "        velo_pts_im[:, 1] = np.round(velo_pts_im[:, 1]) - 1\n",
    "        val_inds = (velo_pts_im[:, 0] >= 0) & (velo_pts_im[:, 1] >= 0)\n",
    "        val_inds = val_inds & (velo_pts_im[:, 0] < im_shape[1]) & (velo_pts_im[:, 1] < im_shape[0])\n",
    "        velo_pts_im = velo_pts_im[val_inds, :]\n",
    "\n",
    "        # project to image\n",
    "        depth = np.zeros(im_shape)\n",
    "        depth[velo_pts_im[:, 1].astype(np.int), velo_pts_im[:, 0].astype(np.int)] = velo_pts_im[:, 2]\n",
    "\n",
    "        # find the duplicate points and choose the closest depth\n",
    "        inds = self.__sub2ind(depth.shape, velo_pts_im[:, 1], velo_pts_im[:, 0])\n",
    "        dupe_inds = [item for item, count in Counter(inds).items() if count > 1]\n",
    "        for dd in dupe_inds:\n",
    "            pts = np.where(inds == dd)[0]\n",
    "            x_loc = int(velo_pts_im[pts[0], 0])\n",
    "            y_loc = int(velo_pts_im[pts[0], 1])\n",
    "            depth[y_loc, x_loc] = velo_pts_im[pts, 2].min()\n",
    "        depth[depth < 0] = 0\n",
    "\n",
    "        if interp:\n",
    "            # interpolate the depth map to fill in holes\n",
    "            depth_interp = self.__lin_interp(im_shape, velo_pts_im)\n",
    "            return depth, depth_interp\n",
    "        else:\n",
    "            return depth\n",
    "        \n",
    "        \n",
    "    def __read_text_lines(self,file_path):\n",
    "        \"\"\" Reads in the ground truth depth file\n",
    "        # adapted from https://github.com/hunse/kitti\n",
    "\n",
    "        Args:\n",
    "            -file_path (string): path to file \n",
    "        Returns:\n",
    "            -lines(list): filenames for the data\n",
    "        \"\"\"\n",
    "        f = open(file_path, 'r')\n",
    "        lines = f.readlines()\n",
    "        f.close()\n",
    "        lines = [l.rstrip() for l in lines]\n",
    "        return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 files missing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:77: RuntimeWarning: divide by zero encountered in true_divide\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 0.99769968,  0.99768972,  0.99772263], dtype=float32),\n",
       " array([ 18.66298866,  18.38948822,  18.45478439], dtype=float32),\n",
       " array([ 22.54434013,  22.20382309,  22.21712494], dtype=float32),\n",
       " array([ 6.76411104,  6.73685884,  6.74664402], dtype=float32),\n",
       " array([ 0.,  0.,  0.], dtype=float32),\n",
       " array([ 0.,  0.,  0.], dtype=float32),\n",
       " array([ 0.,  0.,  0.], dtype=float32))"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EvaluateEigen('pred_disp.npy', test_file_path='', gt_path = '../../data/kitti/').evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
